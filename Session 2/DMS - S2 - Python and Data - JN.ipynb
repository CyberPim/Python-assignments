{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining and Statistics\n",
    "## Session 2 - Data and Python\n",
    "*Peter Stikker - Haarlem, the Netherlands*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous session you got used to a bit of basic Python as a programming language. We've seen how you can do most of your Programming 1 from year 1 with Python in just one session. In this session we focus on dealing with data sets. Once you know this, we can finally actually look at analysing the data starting in session 3.\n",
    "\n",
    "To work with data two very popular libraries will be used: Numpy and Pandas. We'll have a look at how those work but what you'll see here are just tips of icebergs. These libraries are packed with all kinds of useful methods, tricks, etc. \n",
    "\n",
    "You have studied before how to work with data, in year 1 you had Databases SQL, and in the Pandas section in this session we'll use that course as a guideline, similar as we used C# in the previous session.\n",
    "\n",
    "The session finishes with some data cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enjoy this session.\n",
    "\n",
    "p.s. To honor the tradition that a Python course should have a reference to Monty Python, we'll use some Monty Python data as an example in this session :D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous session we discussed different types of collections known to Python core. We saw that an array had the following properties:\n",
    "\n",
    "1. Are different data types allowed in the sequence? NO\n",
    "2. Is the order as the data is given or changed? GIVEN\n",
    "3. Are duplicate values allowed? YES\n",
    "4. Can we make changes? YES\n",
    "\n",
    "It was the first type for which we had to import a library. Below an example of a Python array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import array as arr\n",
    "my_py_array = arr.array('i', [1, 2, 4, 9, 3, 9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An indication that this package/library is native to Python is that we didn't need to install it.\n",
    "\n",
    "However, as you will probably discover one advantage of Python is that many people have created all kinds of useful libraries for others to use. These we will need to install once before we can use them. Numpy is one of those libraries. <a href=\"https://numpy.org/\">Numpy</a> is a library that is almost always listed in a 'top 10 of data mining libraries'. So much so, that often when people talk about an Array in Python they often mean a Numpy Array (or in some cases consider Lists an array).\n",
    "\n",
    "To install a package/library you could use Anaconda, but also you can use Python. Since we're here anyway lets use Python itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*side note: pip is (according to the creator Ian Bicking) a recursive acronym and is short for 'Pip Installs Packages', although I also like the 'Please Install Program'*\n",
    "\n",
    "Next we can import the library, and lets name it np (which is very common to do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a Numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_np_array = np.array([1, 2, 4, 9, 3, 9])\n",
    "type(my_np_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One big advantage of Numpy Arrays is that they are very fast when do perform computations on them, and have a few build in methods already. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_np_array.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_np_array.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also calculations to be performed on each element work as expected..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_np_array * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While if you would do this with a Python Array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_py_array * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you verified earlier if Numpy is indeed mentioned in 'top 10 of data mining libraries', you'll probably seen in the same lists the library Pandas. Pandas relies heavily on Numpy and extends its features. It will be the library we'll use to import the data and work with the data. \n",
    "\n",
    "Its not a Python native library, so if it has not been installed, we can again either use Anaconda or Python itself. A decent explanation on how to install it with Anaconda can be found <a href=\"https://docs.anaconda.com/anaconda/navigator/tutorials/pandas/#:~:text=Pandas%20appears%20as%20a%20package%20available%20for%20installation.\">here</a>, but we'll use Python for now.\n",
    "\n",
    "However, I don't want it to install every time I run this file, but do want to import it. So...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas already installed, only imported\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "print('pandas already installed, only imported')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Getting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although Python has a build in csv reader (similar as the file reader you have seen in C#, see Appendix 1 for an example on this), you will often see (and use) the Pandas version. It is just so much easier to work with. Here's an example of how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\win 10\\OneDrive - Hogeschool Inholland\\Jaar 3\\Data Mining & Statistics\\Python-assignments\\Session 2\\DMS - S2 - Python and Data - JN.ipynb Cell 26'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/win%2010/OneDrive%20-%20Hogeschool%20Inholland/Jaar%203/Data%20Mining%20%26%20Statistics/Python-assignments/Session%202/DMS%20-%20S2%20-%20Python%20and%20Data%20-%20JN.ipynb#ch0000025?line=0'>1</a>\u001b[0m my_data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m\"\u001b[39m\u001b[39mfiles/MontyPython.csv\u001b[39m\u001b[39m\"\u001b[39m, sep\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m;\u001b[39m\u001b[39m'\u001b[39m, skipinitialspace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "my_data = pd.read_csv(\"files/MontyPython.csv\", sep=';', skipinitialspace=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was easy enough. There are plenty of additional parameters that can be used. See the documentation for more details: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\n",
    "\n",
    "So, lets have a look at what we got..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks nice, but is it a set, an array, or..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(my_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah, Pandas has it own datatype known as a DataFrame. This is actually made up of different columns. One way to select a column is simply by its name:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data['Scene']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what data type is this then..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(my_data['Scene'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Pandas DataFrame is made by a few different Pandas Series.\n",
    "\n",
    "You might wonder why knowing the datatype is so important. Well, you will probably be using some other libraries and functions in the near future. It is then important to check if the function you are trying to use can deal with the data type you have. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course we can transform the data into another data type. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list = my_data['Scene'].tolist()\n",
    "\n",
    "type(my_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned at the start of this section, Pandas also has a csv reader. It is not often used anymore, but if you are interested the Appendix shows how it could be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. SQL with Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hope you remember the Databases (SQL) course from year 1. In that course you learned the basics of using SQL to get data in different ways from a database. Also with data mining in Python we want to get data in different ways from a dataset. So lets see how some of those basic SQL commands can be done with Pandas.\n",
    "\n",
    "First we look at **seeing a single field (column)**. If we wanted to see only the *Scene* field from the *my_data* table, we use:\n",
    "\n",
    "<font color=blue>SELECT</font> Scene <br>\n",
    "<font color=blue>FROM</font> my_data\n",
    "\n",
    "With a Pandas Dataframe we can accomplish this by using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'my_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\win 10\\OneDrive - Hogeschool Inholland\\Jaar 3\\Data Mining & Statistics\\Python-assignments\\Session 2\\DMS - S2 - Python and Data - JN.ipynb Cell 41'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/win%2010/OneDrive%20-%20Hogeschool%20Inholland/Jaar%203/Data%20Mining%20%26%20Statistics/Python-assignments/Session%202/DMS%20-%20S2%20-%20Python%20and%20Data%20-%20JN.ipynb#ch0000040?line=0'>1</a>\u001b[0m my_data[\u001b[39m'\u001b[39m\u001b[39mScene\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'my_data' is not defined"
     ]
    }
   ],
   "source": [
    "my_data['Scene']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data.Scene"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With SQL we could **select multiple fields (columns)**, for example to see the *Scene* and *textNr*:\n",
    "\n",
    "<font color=blue>SELECT</font> Scene, textNr <br>\n",
    "<font color=blue>FROM</font> my_data\n",
    "\n",
    "With Pandas Dataframe the same, but note the use of an extra set of brackets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data[['Scene', 'textNr']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for **selecting specific records (rows)**. \n",
    "\n",
    "We didn't discuss this in year one, and it depends a bit on the SQL dialect you use, but one to show the first five rows would be:\n",
    "\n",
    "<font color=blue>SELECT</font> * <br>\n",
    "<font color=blue>FROM</font> my_data <br>\n",
    "<font color=blue>LIMIT</font> 1, 5\n",
    "\n",
    "With Pandas we can do the same using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can of course also **combine the selection of records and fields (rows and columns)**\n",
    "\n",
    "<font color=blue>SELECT</font> Scene, textNr <br>\n",
    "<font color=blue>FROM</font> my_data <br>\n",
    "<font color=blue>LIMIT</font> 1, 5\n",
    "\n",
    "With Pandas this goes easy with 'iloc':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data.iloc[0:5, 0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next SQL clause we discussed was the WHERE, **to filter on a condition for specific records (rows)**. For example, if we only wanted to see the data from 'ARTHUR':\n",
    "\n",
    "<font color=blue>SELECT</font> Scene, textNr <br>\n",
    "<font color=blue>FROM</font> my_data <br>\n",
    "<font color=blue>WHERE</font> Who = <font color=red>'ARTHUR'</font>\n",
    "\n",
    "To accomplish this with Pandas is a two-step procedure. First we create a pandas series with booleans showing if the condition is true or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data['Who'] == 'ARTHUR'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we feed this series of booleans in a request to show the data, et voila:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data[my_data['Who'] == 'ARTHUR']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another topic discussed in SQL lecture 2 from year 1 were the **aggregation functions**. So for example to see the maximum number of text lines:\n",
    "\n",
    "<font color=blue>SELECT</font> <font color=magenta>COUNT</font>(textNr) <br>\n",
    "<font color=blue>FROM</font> my_data <br>\n",
    "\n",
    "With Pandas..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data['textNr'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of aggregate functions we can apply, the four basic ones:\n",
    "\n",
    "* min for minimum\n",
    "* max for maximum \n",
    "* mean for average\n",
    "* count for the number of i.e. count\n",
    "\n",
    "These and a few others can also quickly be obtained on all numeric fields using the *describe* method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple other SQL command we discussed was the ORDER BY, to uhm well **order (or sort) the results**. For example to order by *What* descending:\n",
    "\n",
    "<font color=blue>SELECT</font> * <br>\n",
    "<font color=blue>FROM</font> my_data <br>\n",
    "<font color=blue>ORDER BY</font> What <font color=grey>DESC</font>\n",
    "\n",
    "As you probably suspected, this can also be done with Pandas..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data.sort_values('What', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last SQL clause to discuss is the GROUP BY, since we are often interested in **results per group**. For example we could be interested in the results per actor (Who) and have a few simple counts of texts each had.\n",
    "\n",
    "In SQL that would look like:\n",
    "\n",
    "<font color=blue>SELECT</font> Who, <font color=magenta>COUNT</font>(*) <br>\n",
    "<font color=blue>FROM</font> my_data <br>\n",
    "<font color=blue>GROUP BY</font> Who <br>\n",
    "<font color=blue>ORDER BY</font> 2 <font color=grey>DESC</font>\n",
    "\n",
    "With Pandas we could make one long chain of functions for this, but lets do it step by step first.\n",
    "\n",
    "To let Pandas know we want to group on something, we can use the 'groupby' method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr_who = my_data.groupby(\"Who\")\n",
    "gr_who"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This in itself is just storing the data of each group seperately. Now we can get our counts per group using this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "who_txt_counts = gr_who['textNr'].count()\n",
    "who_txt_counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can sort this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "who_txt_counts_sorted = who_txt_counts.sort_values(ascending=False)\n",
    "who_txt_counts_sorted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And yes, we could have combined all in one long chain of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data.groupby(\"Who\")['textNr'].count().sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do like to use this chaining, but want to somewhat structure it, you can use a set of parenthesis around the entire thing, and then you can use a new line for each new part of the chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    my_data\n",
    "    .groupby(\"Who\")['textNr']\n",
    "    .count()\n",
    "    .sort_values(ascending=False)\n",
    "    .head()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Exercise\n",
    "\n",
    "For this exercise we'll use a datafile that actually comes from another program: R. To understand what the variables mean, you might want to check the documentation about the file, available <a href=\"https://www.rdocumentation.org/packages/datasets/versions/3.6.2/topics/ChickWeight\">here</a>.\n",
    "\n",
    "Shout-out to Mr. Van der Meer for the inspiration for this exercise.\n",
    "\n",
    "*a) Load the chickweight.csv file as a pandas dataframe, and show the first five records*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*b) Show from row numbers 10 to 15 the weight, and diet*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*c) Show the fields/columns weight and time of chick 1.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*d) Determine the maximum weight for each diet*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*e) Determine the max time per chicken. Sort the results low to high. Which chickens seems not to have made it to the last time of measuring?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*f) What are the chick numbers of those with the maximum weight for each diet? (note you might need to search the internet a bit for a solution, hint: transform...)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*g) Show the data of the chicks that did not reach the last time period. Show all their results for their last time period. (note use again the transform)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a decent explanation of transform, see: https://pbpython.com/pandas_transform.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final note on SQL with Python**\n",
    "\n",
    "It is also possible to use true SQL with Python and connect to a database. This is beyond the scope of this course. If you are interested there is a free MOOC available on Coursera.org from IBM that covers this: https://www.coursera.org/learn/sql-data-science/home/welcome. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have heard about the phrase 'garbage in, garbage out'. It means that if the data is not 'good' you cannot get reliable results. \n",
    "\n",
    "Data Cleaning is a study in itself, and some claim it takes about 80% of the time of the data scientist. This claim of 80% is perhaps a bit exagerated (see for example <a href=\"https://blog.ldodds.com/2020/01/31/do-data-scientists-spend-80-of-their-time-cleaning-data-turns-out-no/\">here</a>), but that a lot of time should be spend on this is not debated by anyone in the field.\n",
    "\n",
    "How to clean your data will unfortunately depend on your specific situation. In this chapter just a few techniques are discussed, but there is a lot more. A good starting point to get some more ideas is: https://towardsdatascience.com/the-ultimate-guide-to-data-cleaning-3969843991d4\n",
    "\n",
    "Okay, first lets get some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_df = pd.read_csv(\"StudentStatistics.csv\", sep=';').rename(str.lower, axis='columns') \n",
    "survey_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a quick impression of which columns there actually are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An explanation of each variable is usually also available (or should be). If you haven't collected the data yourself a so-called code book is very useful. This should explain what each variable means and the different options that were available for each variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Setting the proper dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see what data types each column contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So most of them are 'object' which indicates a mix of text and numbers. This is probably the 'slowest' data type to work with. So lets change them to what they should be.\n",
    "\n",
    "The *location* is our first object column. So lets see what the results looked like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_df['location'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only three options. This appears to be a so-called nominal variable. We have clear categories and there is no logical order in the options. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just so we can compare later if this is indeed slower, lets do a group by on location and sum the age for each location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "survey_df.groupby('location')['gen_age'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets change the dtype to *categorical*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_df['location'] = pd.Categorical(survey_df['location'])\n",
    "survey_df['location'].dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the categories are in alphabetical order. Is this now faster?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "survey_df.groupby('location')['gen_age'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results should be faster now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next 'object' is *oaa_objcourse*. First check which scores were given:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_df['oaa_objcourse'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a so-called ordinal variable; a clear logical order in the possible values. We need to specify the values now, and set ordered to True:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_df['oaa_objcourse'] = pd.Categorical(survey_df['oaa_objcourse'], categories = [\"Fully Disagree\", \"Disagree\", \"Neither disagree nor agree\", \"Agree\", \"Fully agree\"], ordered = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_df['oaa_objcourse'].dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully you get the idea now. We could perhaps do a few columns in one go if they have the same values, but we'll leave that as an exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Checking the values and counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very simple check on the data is to produce a frequency table of each variable. It will show for each unique value the number of times it appears in the column. With this you can quickly see if there is perhaps a strange value, or a constant, or an unusual/unexpected high or low count.\n",
    "\n",
    "A basic frequency count can be obtained using **value_counts()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_df['oaa_objcourse'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is another example on the column *gen_classes*. This is the number of classes a student attended:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_df['gen_classes'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice anything strange? \n",
    "\n",
    "Well, it seems we have two students who did not attend any class. Lets take a closer look at them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_df[survey_df['gen_classes'] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmm, very odd. Even though they did not attend any classes, these two students did have a variety of opinions on everything including the classes. How can you have an opinion on a class if you never attended it?\n",
    "\n",
    "We could decide to remove these two students from our data. If you decide to remove or change data always report this and explain why you felt it was appropriate to do so.\n",
    "\n",
    "I usually would make a new copy of the data just in case I regret my decision later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_df2 = survey_df[survey_df['gen_classes'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_df2['gen_classes'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can continue with all of the variables, and would come across:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_df2['gen_age'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice something odd again? You are never too old to learn, but an age of 119 is not likely. Perhaps a typo was made and 19 was meant. If you have the original surveys we could double check, but we don't. Unfortunately we should make this a missing value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we don't want to remove the entire row now, just replace the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_df2['gen_age'] = survey_df2['gen_age'].replace(119, np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a warning, but a warning is not the same as an error. It has actually worked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_df2['gen_age'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we need to be clear in our report that we made this change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. A Visual Inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A visual inspection might also be helpful. For categorical data a bar-chart is useful. Pandas has a plot function (which is actually based on another library Matplotlib's PyPlot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_counts = survey_df2['location'].value_counts().sort_index()\n",
    "my_counts.plot(kind = 'bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing seems out of the ordinary. We made this chart just for a quick inspection. If we wanted to make it look a bit nicer, we can probably better use the Pyplot library from Matplotlib. This is usually abbreviated to 'plt':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    print('matplotlib already installed, only imported')\n",
    "except:\n",
    "    !pip install matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "    print('matplotlib was not installed, installed and imported')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can add all kinds of things, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_counts.plot(kind = 'bar')\n",
    "plt.xlabel(\"location\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xticks(rotation = 45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For numerical fields (columns/variables) we can use something known as a box plot. Pandas also has this available for us in their <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.boxplot.html\">boxplot</a> method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_df2.boxplot('gen_age', grid = False, vert = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what do we see in this diagram?\n",
    "\n",
    "* The vertical line on the left is placed at the minimum value (excluding outliers). So the minimum age was 18.\n",
    "* The box in the middle starts at the so-called first quartile. In this example at 19. This indicates that 25% of all students were 19 or younger.\n",
    "* The vertical line in the box is placed at the median. In this example at 21. This indicates that 50% of all students were 21 or younger.\n",
    "* The end of the box is drawn at the third quartile. In this example at 24. This indicates that 75% of all students were 21 or younger.\n",
    "* The vertical line on the right is placed at the maximum value, excluding outliers. So the maximum age (not counting outliers) was 30\n",
    "* The dot at the far right (O) is a so called outlier. Its a score that seems to be quite unusual.\n",
    "\n",
    "When cleaning your data it are those outliers that are to be looked out for. This could indicate an error in measurement, but not necesairly. An outlier can have quite a big effect on the results later, when we start analyzing the data. \n",
    "\n",
    "Again, if you decide on removing an outlier, make sure you indicate this in your report with an explanation on why you felt it was justified to do so.\n",
    "\n",
    "If you are curious an outlier is a datapoint that is 1.5 times the Inter Quartile Range (IQR) below the first or above the third quartile. The IQR is the difference between the third and first quartile. In the example the IQR = 24 - 19 = 5. So any score below 19 - 1.5 x 5 = 19 - 7.5 = 11.5 would be considered an outlier, or any score above 24 + 1.5 x 5 = 24 + 7.5 = 31.5. The 37 is therefor indeed correctly an outlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1**\n",
    "\n",
    "*1a) Change the dtype for the field mix_intexcel, first check the scores so you can determine the proper dtype for it*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to see the different options that were used for this variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to change the dtype\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*1b) Check the counts of the mix_intexcel field*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*1c) Create an appropriate diagram to visualise the counts of mix_intexcel*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2: Automating the Process**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is some code to adjust the dtype for all fields that use the fully disagree to fully agree scale. \n",
    "\n",
    "Inspect the code carefully so you understand what it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a real copy and not a reference in memory.\n",
    "survey_df3 = survey_df2.copy()\n",
    "\n",
    "#The labels we want to use\n",
    "labels = [\"Fully Disagree\", \"Disagree\", \"Neither disagree nor agree\", \"Agree\", \"Fully agree\", np.nan]\n",
    "\n",
    "#The fields in the data frame\n",
    "fields = survey_df3.columns\n",
    "\n",
    "#Loop over each field\n",
    "for field in fields:\n",
    "    \n",
    "    #check if the data type is 'object'\n",
    "    if survey_df3[field].dtype.name == 'object':\n",
    "        \n",
    "        # get the unique values of the field\n",
    "        values = survey_df3[field].unique()\n",
    "        \n",
    "        # check if those values are in our labels\n",
    "        allin = True\n",
    "        n_values = len(values)\n",
    "        i = 0\n",
    "        while allin and i < n_values:\n",
    "            allin = values[i] in labels\n",
    "            i = i + 1\n",
    "        \n",
    "        # only if the values are in the labels adjust the dtype\n",
    "        if allin:\n",
    "            survey_df3[field] = pd.Categorical(survey_df3[field], categories = [\"Fully Disagree\", \"Disagree\", \"Neither disagree nor agree\", \"Agree\", \"Fully agree\"], ordered = True)\n",
    "            \n",
    "#Check if it worked\n",
    "survey_df3.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*2a) Create a similar code for the variables that start with 'mat_' and run it.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to check the different options that were used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to change the dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*2b) Create a codeblock to show the frequencies of all category dtypes in the dataframe*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*2c) Create a codeblock that shows a bar-chart if the dtype is categorical, and a box-plot if it is a float.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3: Convert to Numbers**\n",
    "\n",
    "Although we already changed for *oaa_objcourse* the dtype from *object* to *category* it can be helpful to change it to numeric values. If we use a coding of for example 0 = fully disagree, 1 = disagree,...4 = fully agree. \n",
    "\n",
    "*3a) Try to change the *oaa_objcourse* field to a numeric one (hint: use replace, or codes). Check if coding indeed assigned fully disagree to 0.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USING REPLACE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USING CODES\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the values have been converted to numbers, we have no way of knowing anymore what each value represented (except for looking through the code). It can therefor be a good idea to store this information somewhere. One possible option that comes to mind is a dictionary with each entry being the variable name, and as a key another dictionary with the code and the corresponding values.\n",
    "\n",
    "*3b) Create a dictionary named **codebook** and add the first entry of 'oaa_objcourse' as a dictionary with the appropriate labels*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if it worked\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seaborn is a powerful data visualisation library that is often used in data mining. It couldn't hurt to play around a bit with it. So...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*3c) Create a bar-chart and a box-plot with the Seaborn library.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix 1: Python's CSV Reader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To import data Python has a CSV reader (similar as the file.writer you hopefully remember from C#). To use this, we do need to import Python's library csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can read files using the 'csv.reader'. Since the file is a bit long, I've added a small if statement so it will only show the first ten rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('MontyPython.csv', newline='') as csvfile:\n",
    "    csv_reader = csv.reader(csvfile, delimiter=';')\n",
    "    i = 0\n",
    "    for row in csv_reader:\n",
    "        i = i + 1\n",
    "        if i < 10:\n",
    "            print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could of course, store the results in a List:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = []\n",
    "\n",
    "with open('MontyPython.csv', newline='') as csvfile:\n",
    "    csv_reader = csv.reader(csvfile, delimiter=';')\n",
    "    for row in csv_reader:\n",
    "        my_data.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see the first 10 rows using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data[0:9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of options with the csv.reader, but usually another reader is used when importing data. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
